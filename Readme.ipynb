{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# FIN411X ChatGPT Co‑Tutor Portfolio Project  \n**Replication package README** (`Readme.ipynb`)\n\nThis notebook documents the structure of the replication package for the study on an AI‑assisted, Python‑based CAPM portfolio project in FIN411X (Investment Analysis). It explains:\n\n- how the repository is organised  \n- which data files are included  \n- what each notebook does  \n- how to reproduce all tables and figures reported in the article\n\nYou can read this notebook directly on GitHub or open it in Jupyter / Google Colab. All paths below are given relative to the root folder of the replication package.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Repository layout\n\nIn the ZIP file supplied with the manuscript, the top‑level folder is:\n\n```text\nReplication_Package/\n  Readme.ipynb                  <-- this notebook (to be added at repo root)\n  code_capm/\n    capm_portfolio_project.ipynb\n    portfolio_analysis.ipynb\n  code_stats/\n    FIN411X_Stats_Analysis.ipynb\n    FIN411X_Make_Tables_and_Figures.ipynb\n    run_FIN411X_–_STATS_SCRIPT.ipynb\n  data/\n    chatgpt_survey_instrument.pdf\n    chatgpt_survey_raw_id.csv\n    summary_all_portfolios.csv\n  documentation/\n    ChatGPT Prompt Library for CAPM & Fixed‑Weight Portfolio (FIN411X Project).docx\n    python_code_notes.docx\n  figures/\n    ai_literacy_vs_performance_scatterplot.png\n    chatgpt_scales_boxplot.png\n    portfolio_returns_histogram.png\n  output/\n    fin411x_correlations.csv\n    fin411x_descriptives.csv\n  tables/\n    table1_portfolio_performance.csv\n    table2_ai_scales_reliability.csv\n    table3_A2_frequency.csv\n    table3_A3_frequency.csv\n    table3_A5_frequency.csv\n    table3_E1_frequency.csv\n    table4_correlations_matrix.csv\n    table5_ai_scales_vs_performance.csv\n```\n\nThe core artefacts for replication are located in the `code_capm/`, `code_stats/`, `data/`, `tables/`, and `figures/` folders.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Software requirements\n\nThe notebooks were designed to run either:\n\n- in **Google Colab** (recommended for quick use), or  \n- in a local **Jupyter** environment (e.g., Anaconda, VS Code, JupyterLab).\n\nAll analyses use standard, widely‑available Python packages.\n\n### 2.1. Python and libraries\n\n- Python 3.9 or later  \n- `numpy`  \n- `pandas`  \n- `yfinance` (for downloading daily price data from Yahoo Finance)  \n- `statsmodels` (for CAPM regressions)  \n- `scipy` (for Cronbach’s alpha and correlations)  \n- `matplotlib` (for figures)  \n- `quantstats` (optional; used for some portfolio diagnostics)\n\nIn a local environment you can install these with, for example:\n\n```bash\npip install numpy pandas yfinance statsmodels scipy matplotlib quantstats\n```\n\n> **Note.** In Google Colab, most packages are pre‑installed; the notebooks include `pip` cells where an additional install is needed.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Data files (`data/`)\n\nAll data files used in the article’s quantitative analyses are included in the `data/` folder.\n\n### 3.1 `summary_all_portfolios.csv`\n\n- One row per student portfolio (N = 30).  \n- Key variables:\n  - `student_id` (pseudonymous ID S01–S30)  \n  - `portfolio_id` (P01–P30)  \n  - `final_value_aed` (final portfolio value, AED)  \n  - `pnl_aed` (absolute profit/loss vs. AED 600,000 initial capital)  \n  - `total_return_pct` (total portfolio return in %)  \n  - `alpha_annual` (annualised CAPM alpha)  \n  - `beta` (CAPM beta)\n\nThis file is the primary source for all portfolio‑level analyses (RQ1) and for linking performance to survey responses (RQ2).\n\n> For **exact replication** of the article, you normally do **not** need to rerun the CAPM portfolio notebook; you can work directly from this CSV.\n\n### 3.2 `chatgpt_survey_raw_id.csv`\n\n- Item‑level responses to the *AI Literacy and Metacognition: ChatGPT Survey*.  \n- Includes the same pseudonymous IDs (`Student_id`) used in `summary_all_portfolios.csv`.  \n- Columns cover:\n  - usage indicators (Section A)  \n  - learning & confidence items (Section B)  \n  - metacognitive strategies items (Section C)  \n  - AI literacy / critical awareness items (Section D)  \n  - behavioural and open‑ended items (Sections E and F).\n\nThe `FIN411X_Stats_Analysis` and `FIN411X_Make_Tables_and_Figures` notebooks use this file to construct the B_scale, C_scale, and D_scale variables.\n\n### 3.3 `chatgpt_survey_instrument.pdf`\n\n- PDF of the full survey instrument for reference.  \n- This file is **not** used directly in the code, but documents the wording and ordering of all items.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Notebooks and their roles\n\nThis section summarises the purpose and inputs/outputs of each notebook.\n\n### 4.1 CAPM / portfolio notebooks (`code_capm/`)\n\n#### `capm_portfolio_project.ipynb`\n\nThis notebook mirrors the student workflow for the CAPM portfolio project. It:\n\n- sets up the Python environment (imports `numpy`, `pandas`, `yfinance`, `statsmodels`, and optional `quantstats`),  \n- downloads daily price data for an assigned 10‑asset portfolio and a benchmark index using `yfinance`,  \n- computes daily asset returns and constructs the portfolio’s daily value based on fixed weights,  \n- calculates daily portfolio returns and corresponding benchmark returns,  \n- builds excess return series for the portfolio and benchmark using a specified risk‑free rate,  \n- estimates CAPM alpha and beta via ordinary least squares (OLS) regression,  \n- computes summary performance indicators (final value, P&L, total return, alpha, beta), and  \n- writes a **one‑row CSV summary** for the chosen `student_id` / `portfolio_id` combination.\n\nIn the teaching context, each student ran this notebook once for their assigned portfolio. For replication, you can either:\n\n- use the already‑aggregated `summary_all_portfolios.csv` in `data/` (recommended), or  \n- rerun this notebook for new or alternative portfolios.\n\n> Because price data are retrieved from Yahoo Finance at runtime, rerunning this notebook in future may yield slightly different CAPM estimates if the data provider revises historical prices. The article’s results are tied to the snapshot captured in `summary_all_portfolios.csv`.\n\n#### `portfolio_analysis.ipynb`\n\nThis instructor‑facing notebook aggregates multiple one‑row portfolio summaries into a single file. It:\n\n- reads individual portfolio summary CSVs (one per student),  \n- concatenates them into a single DataFrame,  \n- standardises column names and ordering, and  \n- exports `summary_all_portfolios.csv` (same structure as the file bundled in `data/`).\n\nFor most users, this notebook is **optional**: you only need it if you wish to regenerate `summary_all_portfolios.csv` from a set of individual portfolio files.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.2 Statistics, tables, and figures (`code_stats/`)\n\n#### `FIN411X_Stats_Analysis.ipynb`\n\nThis notebook performs the main quantitative analyses reported in the Results section. Specifically, it:\n\n1. Loads `data/summary_all_portfolios.csv` (portfolio performance).  \n2. Loads `data/chatgpt_survey_raw_id.csv` (survey responses with `Student_id`).  \n3. Cleans ID fields and merges the two datasets on student ID (S01–S30).  \n4. Constructs composite scale scores:\n   - `B_scale` – learning & confidence (Section B items),  \n   - `C_scale` – metacognitive strategies (Section C items),  \n   - `D_scale` – AI literacy / critical awareness (Section D items).  \n5. Computes Cronbach’s alpha for each scale.  \n6. Produces descriptive statistics for:\n   - portfolio indicators (final value, P&L, total return, alpha, beta), and  \n   - the three AI‑related scales.  \n7. Computes Pearson correlation matrices for:\n   - portfolio performance indicators, and  \n   - relationships between performance and AI‑related scales.\n\nKey outputs:\n\n- Printed tables of descriptive statistics and correlation matrices.  \n- Two CSV files saved to the working directory:\n  - `fin411x_descriptives.csv`  \n  - `fin411x_correlations.csv`  \n\nThese CSVs are also provided in the `output/` folder for convenience.\n\n#### `FIN411X_Make_Tables_and_Figures.ipynb`\n\nThis notebook generates publication‑ready tables and figures from the processed data. It:\n\n- (Re)loads `summary_all_portfolios.csv` and `chatgpt_survey_raw_id.csv`,  \n- ensures consistency of ID fields and scale construction (B_scale, C_scale, D_scale),  \n- formats and exports the main tables reported in the article (Tables 1–5) as CSVs in the `tables/` folder, and  \n- creates the key figures, saving them as PNGs in the `figures/` folder.\n\nConcretely, it produces:\n\n- **Tables (CSV):**  \n  - `tables/table1_portfolio_performance.csv`  \n  - `tables/table2_ai_scales_reliability.csv`  \n  - `tables/table3_A2_frequency.csv`, `tables/table3_A3_frequency.csv`, `tables/table3_A5_frequency.csv`, `tables/table3_E1_frequency.csv`  \n  - `tables/table4_correlations_matrix.csv`  \n  - `tables/table5_ai_scales_vs_performance.csv`  \n\n- **Figures (PNG):**  \n  - `figures/portfolio_returns_histogram.png` (distribution of total returns)  \n  - `figures/ai_literacy_vs_performance_scatterplot.png` (AI literacy vs. total return)  \n  - `figures/chatgpt_scales_boxplot.png` (distribution of the three AI‑related scales)\n\nThese files correspond directly to the tables and figures cited in the Results section.\n\n#### `run_FIN411X_–_STATS_SCRIPT.ipynb`\n\nThis small helper notebook focuses on regenerating the main figures from an already‑merged DataFrame `df` (as produced by `FIN411X_Stats_Analysis`). It:\n\n- assumes `df` is available in memory,  \n- uses `matplotlib` to draw:\n  - the AI literacy vs. total return scatterplot,  \n  - the boxplot of ChatGPT‑related scales, and  \n  - the histogram of portfolio total returns, and  \n- saves each figure to the `figures/` folder.\n\nIf you run `FIN411X_Make_Tables_and_Figures.ipynb`, you normally do **not** need this script; it is included for transparency.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Replication workflow\n\nThis section summarises the minimum steps needed to reproduce the quantitative results, tables, and figures from the article.\n\n### 5.1 Quick replication (recommended)\n\nIf your goal is to reproduce the reported tables and figures **exactly**, you can work directly from the bundled CSV files:\n\n1. **Obtain the repository**  \n   - Download / clone the `chatgpt-cotutor-finance` repository (or the `Replication_Package` folder) to your machine, **or**  \n   - Upload the entire `Replication_Package` folder to Google Drive and open it in Google Colab.\n\n2. **Run the stats analysis notebook**  \n   - Open `code_stats/FIN411X_Stats_Analysis.ipynb`.  \n   - Make sure both `data/summary_all_portfolios.csv` and `data/chatgpt_survey_raw_id.csv` are accessible (in Colab, upload or mount Drive as indicated in the notebook).  \n   - Run all cells from top to bottom.  \n   - This will recompute descriptive statistics, reliability indices, and correlation matrices and save:\n     - `fin411x_descriptives.csv`  \n     - `fin411x_correlations.csv`  \n\n3. **Generate tables and figures**  \n   - Open `code_stats/FIN411X_Make_Tables_and_Figures.ipynb`.  \n   - Run all cells from top to bottom.  \n   - The notebook will:\n     - (re)build B_scale, C_scale, D_scale,  \n     - format and export Tables 1–5 to the `tables/` folder, and  \n     - export the main figures to the `figures/` folder.\n\nAt this point, the contents of `tables/` and `figures/` should match the tables and figures reported in the manuscript (up to rounding/formatting in your text editor).\n\n### 5.2 Optional: Regenerating portfolio summaries\n\nIf you wish to regenerate `summary_all_portfolios.csv` from scratch (for example, to extend the project or explore alternative portfolios):\n\n1. Open `code_capm/capm_portfolio_project.ipynb`.  \n2. Set `student_id` and `portfolio_id` as described in the notebook.  \n3. Run all cells to download prices, compute CAPM statistics, and write a one‑row summary CSV for that portfolio.  \n4. Repeat as needed for additional portfolios.  \n5. Open `code_capm/portfolio_analysis.ipynb` to aggregate individual portfolio summary files into a new `summary_all_portfolios.csv`.  \n6. Move this new CSV into the `data/` folder (or adjust the paths in the stats notebooks accordingly).  \n7. Rerun the stats and table/figure notebooks as in §5.1.\n\n> **Caution.** Because price data are retrieved from Yahoo Finance via `yfinance`, re‑computing portfolios at a later date may yield slightly different values if the provider updates historical prices. For strict replication of the published article, use the `summary_all_portfolios.csv` included in `data/`.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Adapting the materials\n\nThe replication package is designed to be reusable by other instructors and researchers. Some common adaptations include:\n\n- **Changing the portfolio universe**  \n  - Edit the asset tickers and fixed weights in `capm_portfolio_project.ipynb`.  \n  - Regenerate `summary_all_portfolios.csv` as described in §5.2.\n\n- **Modifying the survey or scales**  \n  - If you adapt the AI literacy / metacognition survey, update the column selections and item lists in `FIN411X_Stats_Analysis.ipynb` and `FIN411X_Make_Tables_and_Figures.ipynb` so that B_scale, C_scale, and D_scale reflect your new instrument.\n\n- **Adding new performance metrics**  \n  - Extend the CAPM notebook with additional indicators (e.g., Sharpe ratio, maximum drawdown) using `pandas`, `numpy`, or `quantstats`.  \n  - Store these metrics in `summary_all_portfolios.csv` and incorporate them into `FIN411X_Stats_Analysis` and `FIN411X_Make_Tables_and_Figures` as needed.\n\n- **Using different AI tools**  \n  - The current design frames ChatGPT as a “co‑tutor” for Python and CAPM. You can adapt the reflections and survey items to explore other generative AI systems while keeping the same portfolio and stats pipeline.\n\nIf you modify the notebooks, it is good practice to:\n\n- keep a copy of the original replication package for reference, and  \n- document any changes in a short changelog section at the top of each modified notebook.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Reproducibility notes\n\n- All analyses are deterministic given the fixed CSV files in `data/`; no random seeds are required.  \n- The only potential source of variation arises if you choose to **regenerate** `summary_all_portfolios.csv` from live market data using `yfinance`.  \n- The repository does **not** include raw ChatGPT transcripts; the qualitative analysis is documented in the manuscript and supplementary materials but cannot be fully reproduced from these files for confidentiality reasons.\n\nFor any issues running the notebooks, check that:\n\n1. The current working directory in Jupyter/Colab is the root of the replication folder, and  \n2. All required Python libraries are installed and importable.\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}